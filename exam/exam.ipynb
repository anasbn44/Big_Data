{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cette session Spark est configurée avec les paramètres suivants :\n",
    "\n",
    "- `appName('examen')`: Cette méthode définit le nom de l'application Spark en cours.\n",
    "\n",
    "- `.config(\"pyspark.jars\", \"C:/Users/anaso/anaconda3/envs/newConda/Lib/site-packages/pyspark/jars\")`: On a spécifié le chemin vers le répertoire contenant les fichiers JAR nécessaires pour PySpark pour acceder a mySQL connector.\n",
    "\n",
    "- `.getOrCreate()`: Cette méthode crée une session Spark si elle n'existe pas déjà, ou bien récupère la session existante si elle est déjà active."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('examen') \\\n",
    "    .config(\"pyspark.jars\", \"C:/Users/anaso/anaconda3/envs/newConda/Lib/site-packages/pyspark/jars\") \\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `mysql_props` : C'est un dictionnaire contenant les propriétés de connexion à la base de données MySQL. Les clés du dictionnaire incluent le nom d'utilisateur (`user`), le mot de passe (`password`), et le pilote JDBC à utiliser (`driver`).\n",
    "\n",
    "- `mysql_url` : C'est l'URL de connexion à la base de données MySQL. Dans cet exemple, la base de données est hébergée localement (`localhost`) sur le port standard MySQL (`3306`). La base de données utilisée est nommée `db_exam`.\n",
    "\n",
    "- `df_clients = spark.read.jdbc(url=mysql_url, table=\"clients\", properties=mysql_props)` : Cette ligne de code charge les données de la table `clients` de la base de données MySQL spécifiée dans le DataFrame `df_clients`. Spark utilise le pilote JDBC pour établir la connexion, récupérer les données de la table `clients`, et les charger dans le DataFrame Spark.\n",
    "\n",
    "- `df_commandes = spark.read.jdbc(url=mysql_url, table=\"commandes\", properties=mysql_props)` : De manière similaire que df_clients, cette instruction charge les données de la table `commandes` de la base de données MySQL dans le DataFrame `df_commandes`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mysql_props = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "mysql_url = \"jdbc:mysql://localhost:3306/db_exam\"\n",
    "\n",
    "df_clients = spark.read.jdbc(url=mysql_url, table=\"clients\", properties=mysql_props)\n",
    "df_commandes = spark.read.jdbc(url=mysql_url, table=\"commandes\", properties=mysql_props)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `df_clients.createOrReplaceTempView(\"clients\")` : Cette methode crée une vue temporaire nommée \"clients\" à partir du DataFrame `df_clients`. Cette vue temporaire peut être utilisée pour exécuter des requêtes SQL sur les données de ce DataFrame.\n",
    "\n",
    "- `df_commandes.createOrReplaceTempView(\"commandes\")` : De manière similaire, on crée une vue temporaire nommée \"commandes\" à partir du DataFrame `df_commandes`. Cela permet d'accéder aux données de la table `commandes` via des requêtes SQL dans Spark SQL en utilisant cette vue temporaire.\n",
    "\n",
    "Une fois ces vues temporaires créées, on peut effectuer des opérations SQL telles que les sélections, les filtrages, les agrégations, etc., sur les DataFrames associés en utilisant la syntaxe SQL dans Spark SQL. Ces vues temporaires servent de pont entre les DataFrames Spark et les requêtes SQL pour effectuer des analyses ou des manipulations de données."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_clients.createOrReplaceTempView(\"clients\")\n",
    "df_commandes.createOrReplaceTempView(\"commandes\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Affichage du nombre total de commandes passées par tous les clients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|TotalOrders|\n",
      "+-----------+\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_orders = spark.sql(\"SELECT COUNT(*) AS TotalOrders FROM commandes\")\n",
    "total_orders.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Affichage du client qui a dépensé le plus (en terme de montant)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|   NOM|TotalSpent|\n",
      "+------+----------+\n",
      "|Moulay|    700.49|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client_max_spent = spark.sql(\"\"\"\n",
    "    SELECT c.NOM, SUM(co.MONTANT_TOTAL) AS TotalSpent\n",
    "    FROM clients c\n",
    "    JOIN commandes co ON c.ID_CLIENT = co.ID_CLIENT\n",
    "    GROUP BY c.NOM\n",
    "    ORDER BY TotalSpent DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "client_max_spent.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Affichage de la moyenne des dépenses par client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|     NOM|AvgSpending|\n",
      "+--------+-----------+\n",
      "|  Moulay|    350.245|\n",
      "|BenAhmed|    225.625|\n",
      "|  Sanlam|       75.2|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_spending_per_client = spark.sql(\"\"\"\n",
    "    SELECT c.NOM, AVG(co.MONTANT_TOTAL) AS AvgSpending\n",
    "    FROM clients c\n",
    "    JOIN commandes co ON c.ID_CLIENT = co.ID_CLIENT\n",
    "    GROUP BY c.NOM\n",
    "\"\"\")\n",
    "avg_spending_per_client.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
